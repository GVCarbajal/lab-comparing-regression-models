{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a3def8",
   "metadata": {},
   "source": [
    "# Lab | Comparing regression models\n",
    "\n",
    "For this lab, we will be using the same dataset we used in the previous labs. We recommend using the same notebook since you will be reusing the same variables you previous created and used in labs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf53ceb",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "1) In this final lab, we will model our data. Import sklearn train_test_split and separate the data.\n",
    "\n",
    "2) Try a simple linear regression with all the data to see whether we are getting good results.\n",
    "\n",
    "3) Great! Now define a function that takes a list of models and train (and tests) them so we can try a lot of them without repeating code.\n",
    "\n",
    "4) Use the function to check LinearRegressor and KNeighborsRegressor.\n",
    "\n",
    "5) You can check also the MLPRegressor for this task!\n",
    "\n",
    "6) Check and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41305798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8822182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('files_for_lab/we_fn_use_c_marketing_customer_value_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055733ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9a424",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770da907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [df.columns[col_name].lower().replace(' ','_') for col_name in range(len(df.columns))]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('customer', inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f153f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 'effective_to_date' is not in the right format:\n",
    "\n",
    "df['effective_to_date'] = pd.to_datetime(df['effective_to_date'], errors='coerce')\n",
    "\n",
    "\n",
    "# Column 'policy' contains crossed data from 'policy_type'. Erase it:\n",
    "\n",
    "for i in range(len(df['policy'])):\n",
    "    df['policy'][i] = df['policy'][i].split()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the consistency of categorical values\n",
    "\n",
    "for col in df.select_dtypes(np.object).columns:\n",
    "    print(\"Unique entires for\", col.upper(), \"are:\", df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3870c94",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda0dfa",
   "metadata": {},
   "source": [
    "## Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713331fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64493264",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select_dtypes(np.number).columns:\n",
    "    sns.distplot(df[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609c434",
   "metadata": {},
   "source": [
    "## Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc749da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(np.object).columns:\n",
    "    sns.countplot(x=df[col]) #, hue=df['education']\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = df.corr(method='spearman')\n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a correlation matrix to locate possible correlation right away\n",
    "\n",
    "mask = np.zeros_like(corr_mat)\n",
    "mask[np.triu_indices_from(mask)] = True # Mask to hide the repeated half of the matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12)) # this will set the width and height of the plot\n",
    "sns.set_context('paper') #This affects things like the size of the labels, lines, and other elements of the plot\n",
    "ax = sns.heatmap(corr_mat, mask=mask, annot=True, cmap='jet') # Heatmap declaration\n",
    "ax.set_title('Multi-collinearity of customer features') #Title for the graph\n",
    "plt.show()\n",
    "# plt.savefig('heatmap.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "sns.lineplot(x=df['effective_to_date'].unique(), y=df['effective_to_date'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84592e",
   "metadata": {},
   "source": [
    "# Data cleaning and wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7940680c",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "1) We will start with removing outliers. So far, we have discussed different methods to remove outliers. Use the one you feel more comfortable with, define a function for that. Use the function to remove the outliers and apply it to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d52f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show outliers\n",
    "\n",
    "for column in df.select_dtypes(include='number').columns:\n",
    "    sns.boxplot(df[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63530ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "\n",
    "def remove_outliers(df, threshold=1.5, in_columns=df.select_dtypes(include='number').columns, skip_columns=[]):\n",
    "    for column in in_columns:\n",
    "        if column not in skip_columns:\n",
    "            upper = np.percentile(df[column],75)\n",
    "            lower = np.percentile(df[column],25)\n",
    "            iqr = upper - lower\n",
    "            upper_limit = upper + threshold * iqr\n",
    "            lower_limit = lower - threshold * iqr\n",
    "            df = df[(df[column]>=lower_limit) & (df[column]<=upper_limit)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11941d",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "2) Create a copy of the dataframe for the data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not reduce your data too much, but play with the parameters, \n",
    "# so after a better analysis you can choose which columns to drop outliers from\n",
    "\n",
    "print(len(df), \"original columns\")\n",
    "\n",
    "df1 = df.copy()\n",
    "\n",
    "df1 = remove_outliers(df1, threshold=2.5)\n",
    "\n",
    "print(len(df1), \"columns after removing outliers\")\n",
    "print(round((1-(len(df1)/len(df)))*100,1), \"% of total data loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers on the most important columns with a tighter threshold\n",
    "\n",
    "print(len(df1), \"initial columns\")\n",
    "\n",
    "df2 = remove_outliers(df1, threshold=1.5, in_columns=['income', 'monthly_premium_auto', 'total_claim_amount'])\n",
    "# now filter the outliers on the more important columns with a tighter threshold\n",
    "\n",
    "print(len(df2), \"columns after removing outliers\")\n",
    "print(round((1-(len(df2)/len(df)))*100,1), \"% of total data loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b313dd2",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "3) Normalize the continuous variables. You can use any one method you want.\n",
    "\n",
    "4) Encode categorical variables.\n",
    "\n",
    "5) The time variable can be useful. Try to transform its data into a useful one. Hint: Day week and month as integers might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58cc65f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform dates into useful data\n",
    "\n",
    "df2['day'] = df2['effective_to_date'].dt.day\n",
    "\n",
    "df2['week'] = df2['effective_to_date'].dt.week\n",
    "\n",
    "df2['month'] = df2['effective_to_date'].dt.month\n",
    "\n",
    "df2.drop(['effective_to_date'], axis=1, inplace=True)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode ordinal variables\n",
    "\n",
    "df2[\"coverage\"] = df2[\"coverage\"].map({\"Basic\" : 0, \"Extended\" : 1, \"Premium\" : 2})\n",
    "\n",
    "df2[\"education\"] = df2[\"education\"].map({\n",
    "    \"High School or Below\" : 0, \"College\" : 1, \"Bachelor\" : 2, \"Master\" : 3, \"Doctor\" : 4})\n",
    "\n",
    "df2[\"location_code\"] = df2[\"location_code\"].map({\"Rural\" : 0, \"Suburban\" : 1, \"Urban\" : 2})\n",
    "\n",
    "df2[\"vehicle_size\"] = df2[\"vehicle_size\"].map({\"Small\" : 0, \"Medsize\" : 1, \"Large\" : 2})\n",
    "\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-y split\n",
    "\n",
    "y = df2['total_claim_amount'] # place df1 or df2 if you want to use filtered datasets\n",
    "X = df2.drop(['total_claim_amount'], axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695fc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical values\n",
    "\n",
    "X_num = X.select_dtypes(include='number')\n",
    "X_cat = X.select_dtypes(exclude='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a98482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding categorical variables\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='error', drop='first') #drop one column for efficiency. It can be deduced\n",
    "X_cat_encoded = encoder.fit_transform(X_cat).toarray()\n",
    "X_cat_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cffa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat DataFrames\n",
    "\n",
    "column_names = list(X_num.columns) # get list of numerical column names\n",
    "column_names.extend(list(encoder.get_feature_names())) # add list of dummified categorical column names\n",
    "\n",
    "X_numcat = np.concatenate([X_num, X_cat_encoded], axis=1)\n",
    "X_ready = pd.DataFrame(data=X_numcat, index=X.index, columns=column_names)\n",
    "X_ready.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a5f96",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "6) Since the model will only accept numerical data, check and make sure that every column is numerical, if some are not, change it using encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913951db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ready.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
